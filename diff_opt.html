
<style>
a {
text-decoration: none;
}
</style> 

<!DOCTYPE html>
<html lang="english">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-41105501-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-41105501-1');
  </script>

  <meta charset="utf-8" />
  <title>Differentiating an optimal solution</title>
  <meta name="author" content="Mario Souto" />
  <link rel="stylesheet" type="text/css" href="/theme/tufte-css/tufte.css" />
  <link rel="stylesheet" type="text/css" href="/theme/css/main.css" />
  <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png" />
  <link rel="shortcut icon" href="/static/favicon.ico" />
  <link rel="icon" type="image/png" href="/static/favicon.png" />

  <!-- OpenGraph Info -->


  <style type="text/css">
    .blue {
      color: blue;
    }
  </style>

<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  tex2jax: {
  inlineMath: [['$','$'], ['\\(','\\)']],
  processEscapes: true},
  jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
  extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
  TeX: {
  extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
  equationNumbers: {
  autoNumber: "AMS"
  }
  }
  });
</script>
  
  
</head>

<body>
<header style="text-decoration:none">
  <h1 class="websitetitle" style="text-decoration:none"><a style="text-decoration:none" style="text-decoration:none" href="">Resolvent by Mario Souto</a></h1>
  <!-- <nav class="navmenu" id="navmenu">
    <li><a style="text-decoration:none" class="navitemlink" href="/archives.html">Archive</a></li>
    <li><a style="text-decoration:none" class="navitemlink" href="/pages/about.html"></a></li>
    <li><a style="text-decoration:none" class="navitemlink" href="/pages/publications.html"></a></li>
   </nav> -->
   <nav class="navmenu" id="navmenu" style="text-decoration:none">
    <li><a style="text-decoration:none" style="text-decoration:none" class="navitemlink" href="/pages/about.html">Bio</a></li>
    <li><a style="text-decoration:none" style="text-decoration:none" class="navitemlink" href="/pages/publications.html">Publications</a></li>
    <li><a style="text-decoration:none" style="text-decoration:none" class="navitemlink" href="/">Blog</a></li>
    <!-- <li><a style="text-decoration:none" class="navitemlink" href="/pages/teaching.html">Teaching</a></li>
    <li><a style="text-decoration:none" class="navitemlink" href="/archives.html">Archive</a></li> -->
   </nav>
 </header>
<article>
<style>
a {
text-decoration: none;
}
</style> 

<header style="text-decoration:none" class="post-header">
<h1> <a rel="bookmark" style="text-decoration:none"
   title="Differentiating an optimal solution «Resolvent by Mario Souto»"
   href="/diff_opt.html">
   Differentiating an optimal solution
</a>
</h1>
<p class="subtitle" style="text-decoration:none"><time datetime="2019-10-02T12:00:00+02:00">Wed 02 October 2019</time><label for="diff_opt" class="margin-toggle"> ⊕</label><input type="checkbox" id="diff_opt" class="margin-toggle" /><span class="marginnote">Category: <a href="/category/optimization-differentiable-programming.html">Optimization, Differentiable Programming</a><br />
</span></p>

<!-- <address class="vcard author">
        By                 <a class="url fn" href="/author/mario-souto.html">Mario Souto</a>  (<a href="https://"></a>)
</address>
 -->

</header><p>def</p>
<p>Most of the ideas presented in this post are based on the work of <a href="http://bamos.github.io/">Brandon Amos</a> and <a href="http://zicokolter.com/">Zico Kolter</a>. With that being said, I highly recomend the reader to take a look into the <a href="https://arxiv.org/pdf/1703.00443.pdf">OptNet</a> paper and Brandon's Ph.D. <a href="http://reports-archive.adm.cs.cmu.edu/anon/anon/usr/ftp/2019/CMU-CS-19-109.pdf">thesis</a>. More recently, <a href="http://web.stanford.edu/~boyd/">Stephen Boyd</a> group has also made a very insteresting <a href="https://arxiv.org/pdf/1904.09043.pdf">paper</a> on the subject.</p>
<p><br/></p>
<h1>Jacobians and partial derivatives</h1>
<p>Before we can start to look at optimization problems we need to introduce some useful concepts. Let $F : \mathbb{R}^n \rightarrow \mathbb{R}^m$ be a differentiable multivariate mapping. The correspondent Jacobian is a matrix containing the partial derivatives. In the literature the Jocabian can have different notations, such as $J_F$ or $D_x (F)$, in this post we refer to the Jacobian as $\partial F / \partial x \in \mathbb{R}^{m \times n}$, as given by 
\[
\begin{equation}
\begin{aligned}
&amp; \frac{\partial F}{\partial x} = \begin{bmatrix}
    \partial F_1 / \partial x_1 &amp; \partial F_1 / \partial x_2 &amp; \dots  &amp; \partial F_1 / \partial x_n \newline
    \vdots  &amp; \vdots  &amp; &amp; \vdots \newline
    \partial F_m / \partial x_1 &amp; \partial F_m / \partial x_2 &amp; \dots  &amp; \partial F_m / \partial x_n
\end{bmatrix}.
\end{aligned}
\end{equation}\nonumber
\]</p>
<p>We can also extend this definition to mappings from matrices to matrices. In that case, the mapping $F : \mathbb{R}^{p \times q} \rightarrow \mathbb{R}^{m \times n}$ has a Jacobian $\partial F / \partial x \in \mathbb{R}^{m n \times p q}$ of the form
\[
\begin{equation}
\begin{aligned}
&amp; \frac{\partial F}{\partial x} = \begin{bmatrix}
    \partial F_{1,1} / \partial X_{1,1} &amp; \partial F_{1,1} / \partial X_{2,1} &amp; \dots &amp; \partial F_{1,1} / \partial X_{p,1} &amp; \partial F_{1,1} / \partial X_{1,2} &amp; \dots  &amp;  \partial F_{1,1} / \partial X_{p,q} \newline
    \vdots  &amp; &amp; \vdots &amp; &amp; \vdots &amp; &amp; \vdots \newline
    \partial F_{m,1} / \partial X_{1,1} &amp; \partial F_{m,1} / \partial X_{2,1} &amp; \dots &amp; \partial F_{m,1} / \partial X_{p,1} &amp; \partial F_{m,1} / \partial X_{1,2} &amp; \dots  &amp;  \partial F_{m,1} / \partial X_{p,q} \newline
    \vdots  &amp; &amp; \vdots &amp; &amp; \vdots &amp; &amp; \vdots \newline
    \partial F_{m,n} / \partial X_{1,1} &amp; \partial F_{m,n} / \partial X_{2,1} &amp; \dots &amp; \partial F_{m,n} / \partial X_{p,1} &amp; \partial F_{m,n} / \partial X_{1,2} &amp; \dots  &amp;  \partial F_{m,n} / \partial X_{p,q}
\end{bmatrix},
\end{aligned}
\end{equation}\nonumber
\]
which is a vectorized version of the usual Jacobian as in $\frac{\partial \text{vec}(F(X))}{\partial \text{vec}(X)}$.</p>
<h2>The implicit function theorem</h2>
<p>Let $F : \mathbb{R}^{n + m} \rightarrow \mathbb{R}^n$ be a differentiable mapping and $(y, \theta) \in \mathbb{R}^n \times \mathbb{R}^m$ be a root of $F$, i.e. $F(y, \theta) = 0$. If the partial Jacobian $J_{F,y}$ is invertible, then there exists a neighborhood $U \subset \mathbb{R}^m$ containing $\theta$ such that $y = z(\theta)$ is a unique, continuous and differentiable function on $U$. With this parametrization, we have that
\[
\begin{equation}
\begin{aligned}
&amp; F(z(\theta), \theta) = 0 \hspace{0.2cm} \forall \hspace{0.2cm} \theta \in U,
\end{aligned}
\end{equation}\nonumber
\]
and by taking the derivative of both sides
\[
\begin{equation}
\begin{aligned}
&amp; \frac{\partial F(z(\theta), \theta)}{\partial \theta} = \frac{\partial F(z(\theta), \theta)}{\partial \theta} + \frac{\partial F(z(\theta), \theta)}{\partial z(\theta)} \frac{\partial z(\theta)}{\partial \theta} = 0.
\end{aligned}
\end{equation}\nonumber
\]
As a consequence we can find $\partial z(\theta) / \theta \in \mathbb{R}^{n \times m}$ by solving 
\[
\begin{equation}
\begin{aligned}
&amp; \frac{\partial z(\theta)}{\theta_j} = - \left( \frac{\partial F(z(\theta), \theta)}{\partial z(\theta)} \right)^{-1} \frac{\partial F(z(\theta), \theta)}{\partial \theta_j} \hspace{0.2cm} \forall \hspace{0.2cm} j = 1, \dots, m.
\end{aligned}\label{eq_implicit}
\end{equation}
\]</p>
<p><br/></p>
<h1>Differentiating the optimal solution</h1>
<p>Now lets see how we can use of the implicit function theorem as a tool for differentiating an optimal solution of an optimization problem. Consider the case of an equality constrained convex quadratic minimization problem that is parametrized by a variable $\theta \in \mathbb{R}^k$.
\[
\begin{equation}
\begin{aligned}
&amp; \underset{x \in \mathbb{R}^n}{\text{minimize}}
&amp; &amp; (1/2) x^T P(\theta) x + q(\theta)^T x \newline
&amp; \text{subject to}
&amp;&amp; A(\theta) x = b(\theta).
\end{aligned}\label{eq_qp}
\end{equation}\nonumber
\]
The KKT optimality conditions in matrix form for this problem are
\[
\begin{equation}
\begin{aligned}
&amp; \begin{bmatrix}
P(\theta) &amp; A(\theta)^T  \newline
A(\theta) &amp; 0
\end{bmatrix} \begin{pmatrix} x \newline \nu \end{pmatrix} = \begin{pmatrix} -q(\theta) \newline b(\theta) \end{pmatrix}.
\end{aligned}\label{qp_kkt}
\end{equation}\nonumber
\]
Let $z = \begin{pmatrix} x \newline \nu \end{pmatrix} \in \mathbb{R}^{m + n}$, we can express the KKT conditions in terms of the mapping $F : \mathbb{R}^{m + n + k} \rightarrow \mathbb{R}^{m + n}$ as the following
\[
\begin{equation}
\begin{aligned}
&amp; F(z(\theta), \theta) = \underbrace{\begin{bmatrix}
P(\theta) &amp; A(\theta)^T  \newline
A(\theta) &amp; 0
\end{bmatrix}}_{Q \in \hspace{0.01cm} \mathbb{S}^{m + n}} z(\theta) + \begin{pmatrix} q(\theta) \newline -b(\theta) \end{pmatrix} = 0.
\end{aligned}
\end{equation}\nonumber
\]
Firstly, we note that $\frac{\partial F(z(\theta), \theta)}{\partial z(\theta)} = Q$. By applying the implicit function theorem (\ref{eq_implicit}) we have that 
\[
\begin{equation}
\begin{aligned}
&amp; \frac{\partial z(\theta)}{\theta_j} = - Q^{-1} \frac{\partial F(z(\theta), \theta)}{\partial \theta_j} \hspace{0.2cm} \forall \hspace{0.2cm} j = 1, \dots, m.
\end{aligned}\label{forward_diff} 
\end{equation}
\]</p>
<p>In order to understand how this would work in practice let's explore two different kinds of parametrizations.</p>
<h2>Case where only $b$ is parametrized in terms of $\theta$.</h2>
<p>Suppose that $b$ is a map of $\theta \in \mathbb{R}^m$. In this case, we have that
\[
\begin{equation}
\begin{aligned}
&amp; \frac{\partial F(z(\theta), \theta)}{\partial \theta} = - \begin{bmatrix} 0 \newline \frac{\partial b(\theta)}{\partial \theta} \end{bmatrix} \in \mathbb{R}^{m + n \times m}.
\end{aligned}
\end{equation}\nonumber
\]
Let the map be defined as $b(\theta) = I_{(m \times m)} \theta$, in order to find $\partial F / \partial \theta$ we need to solve all $m$ systems
\[
\begin{equation}
\begin{aligned}
&amp; \frac{\partial z(\theta)}{\partial \theta} = Q^{-1} \begin{bmatrix} 0 \newline I_{(m \times m)} \end{bmatrix}.
\end{aligned}\label{forward_b}
\end{equation}
\]</p>
<h2>Case where only $A$ is parametrized in terms of $\theta$.</h2>
<p>This case is slightly more tricky since $A$ is a $m \times n$ matrix. The partial derivative is then given by 
\[
\begin{equation}
\begin{aligned}
&amp; \frac{\partial F(z(\theta), \theta)}{\partial \theta} = \begin{bmatrix} \frac{\partial A^T(\theta) \nu}{ \partial A(\theta)} \frac{\partial A(\theta)}{\partial \theta} \newline \frac{\partial A(\theta) x}{ \partial A(\theta)} \frac{\partial A(\theta)}{\partial \theta} \end{bmatrix} \in \mathbb{R}^{m + n \times m n}.
\end{aligned}
\end{equation}\nonumber
\]</p>
<p>Lets define the parametrization of $A$ in terms of a simple function such as $A(\theta) = \mathbb{1}_{m \times n} \odot \theta$ where $\theta \in \mathbb{R}^{m \times n}$. Similarly to the previous case, to obtain $\partial F / \partial \theta$ we need to solve all $m n$ systems</p>
<p>\[
\begin{equation}
\begin{aligned}
&amp; \frac{\partial z(\theta)}{\partial \theta} = - Q^{-1} \begin{bmatrix} I_{(n \times n)} \otimes \nu^T \newline x^T \otimes I_{(m \times m)} \end{bmatrix}.
\end{aligned}\label{forward_A}
\end{equation}
\]</p>
<p><br/></p>
<h1>Backpropagation through the optimal solution</h1>
<p>So far we have explicitely build the Jacobian matrix $\partial z(\theta) / \partial \theta$. For several resons this might not be the best strategy. Firstly the Jacobian might be too big to be stored in memory and secondly building the Jacobian requires us to solve $k$ linear systems. As we will see in the following, if $k$ is bigger than the size of $z$ it might be more efficent to compute the adjoint of the Jacobian.</p>
<p>It is easy to see that you will need x passes through...</p>
<p>Instead of explicitely building the Jacobian we can</p>
<p>adjoint</p>
<p>\[
\begin{equation}
\begin{aligned}
&amp; \left( \frac{\partial \ell (z(\theta))}{\partial \theta} \right)^T = - \left( \frac{\partial F(z(\theta), \theta)}{\partial \theta} \right)^T \left( Q^{-1} \right)^T \left( \frac{\partial \ell (z(\theta))}{\partial z(\theta)} \right)^T.
\end{aligned}\label{backprop}
\end{equation}
\]
If we refer to $\omega \in \mathbb{R}^{m + n \times q}$ as the solution of all $m + n$ systems as in
\[
\begin{equation}
\begin{aligned}
&amp; \omega = - \left( Q^{-1} \right)^T \left( \frac{\partial \ell (z(\theta))}{\partial z(\theta)} \right)^T,
\end{aligned}
\end{equation}\nonumber
\]
we can express the gradients in terms of $\omega$. In this case, instead of solving $k$ linear systems we need to solve $m + n$. Let's return the previous example to see how that would play out.</p>
<h2>Backpropagation w.r.t. $b$</h2>
<p>By applying the formula (\ref{forward_b}) to the backward propagation equation in (\ref{backprop}), we get the simple formula of the derivative of $\ell$ with respect to $b$.
\[
\begin{equation}
\begin{aligned}
&amp; \frac{\partial \ell (z(b))}{\partial b} = \begin{bmatrix} 0 \newline w \end{bmatrix}
\end{aligned}
\end{equation}\nonumber
\]</p>
<p><br/></p>
<h1>Differentiable programming</h1>
<p>The idea of make optimization programs differetiable can be seen as a particular instance of the effort of making everything differentiable. If a computer program is diferentiable one optimize </p>
<p><a href="https://en.wikipedia.org/wiki/Differentiable_programming"><em>differentiable programming</em></a>. In a brief <a href="https://www.facebook.com/yann.lecun/posts/10155003011462143">post</a>, Yann LeCun has called atention to differentiable programming as a generalization of deep learning:
<div class="epigraph">
<blockquote>
<p>
OK, Deep Learning has outlived its usefulness as a buzz-phrase.
Deep Learning est mort. Vive Differentiable Programming! <br/>
Yeah, Differentiable Programming is little more than a rebranding of the modern collection Deep Learning techniques, the same way Deep Learning was a rebranding of the modern incarnations of neural nets with more than two layers. <br/>
But the important point is that people are now building a new kind of software by assembling networks of parameterized functional blocks and by training them from examples using some form of gradient-based optimization.</p>
<h2>Further reading</h2>
<p>For the ones that are interested in the subject I would highly recomend the following reads:</p>
<ul>
<li>
<p><a href="https://timvieira.github.io/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/">Backprop is not just the chain rule</a> by Tim Vieira;</p>
</li>
<li></li>
</ul>
</article>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
// (function() { // DON'T EDIT BELOW THIS LINE
// var d = document, s = d.createElement('script');
// s.src = 'https://isaythings.disqus.com/embed.js';
// s.setAttribute('data-timestamp', +new Date());
// (d.head || d.body).appendChild(s);
// })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<!-- <footer>
  <p>Powered by <a style="text-decoration:none" href="http://pelican.readthedocs.org">Pelican</a></p>
</footer> -->
<!-- <script id="dsq-count-scr" src="//isaythings.disqus.com/count.js" async></script> -->
</body>

</html>